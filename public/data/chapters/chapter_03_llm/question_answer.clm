abstract:
  purpose: Answer questions using a local LLM
  description: 'Uses Ollama with llama3:latest to answer questions in a helpful manner.

    Demonstrates chat mode with system prompt.

    '
  inputs:
    question:
      type: string
      description: Question to answer
  outputs:
    answer:
      type: string
      description: LLM-generated answer
  preconditions:
  - Ollama service is running
  - llama3:latest model is available
  postconditions:
  - Returns a text answer
concrete:
  runtime: llm
  provider: ollama
  model: llama3:latest
  llm_config:
    system_prompt: 'You are a helpful AI assistant. Answer questions clearly and accurately.

      If you don''t know something, say so honestly.

      '
    temperature: 0.7
    max_tokens: 500
    response_format: text
  input_type: mcard
  output_type: mcard
  process_type: custom
balanced:
  test_cases:
  - given: What is a monad in functional programming?
    when:
      operation: answer
    then:
      result_contains: function
  expectations:
    performance:
      max_time_ms: 60000
    quality:
      accuracy: high
